from bs4 import BeautifulSoup
import numpy as np
import requests
import re
from wikiscrape import wiki
import os
import seaborn as sns
from collections import Counter
import nltk
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer
from sklearn.feature_selection import chi2
import pickle





os.chdir('/Users/lfairgrievepark12/Documents/Personal_Projects/Wikipedia_Neutrality/')
def linkgen(template, count='all'):
    '''
    Pulls the links to wikipedia articles that are tagged with some user defined 
    template. Good for generating "flagged" data to train for detection (biased articles, etc).

    template: The tag instructing what type of flagged article links to pull (ie. Autobiography),
    The full list is given here: https://en.wikipedia.org/wiki/Wikipedia:Template_index

    count: How many links you want. Default is all the links up to 5000 (there probably won't 
    be that many)
    '''

    page = requests.get("https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Template:"+template+"&limit=5000")
    soup = BeautifulSoup(page.content, 'html.parser')

    links=[]
    if count == 'all':
        for tag in soup.find_all("a", href=re.compile("/wiki/"))[3:]: #first three links on the page are not relevant articles, so they're dropped
            links.append(tag['href'])
        index = [i for i, s in enumerate(links) if 'Talk:' in s or 'User:' in s][0] # all articles after the first "user:" or "talk:" article are dropped
        links = links[:index]
        links =  [i[6:] for i in links]

    else:
        for tag in soup.find_all("a", href=re.compile("/wiki/"))[3:]:
            links.append(tag['href'])
        index = [i for i, s in enumerate(links) if ':' in s][0]
        links = links[:index]
        if count > len(links):
            print('Error: You requested more links than exist for this template')
            return None
        links = links[:count]
        links =  [i[6:] for i in links]
        
        
    return links



def datagen(savefile, flaggedlinks, unflaggedcat, dropstubs=True, dataratio=1):
    '''
    Generates both flagged (biased, autobiography) and unflagged (random article, random in section etc)
    text data as a csv file for analysis

    savefile: File to save data as (must  end in .csv)
    flaggedlinks: List of links/article titles generated by linkgen function
    unflaggedcat: Category to generate random unflagged data from (ex. "Special:RandomInCategory/Living_people")
    dropstubs: Drop any article marked as a stub by wikipedia
    dataratio: ratio of flagged articles generated to unflagged articles
    '''
    with open(savefile, 'w') as export:
        for link in flaggedlinks:
            print(link)
            # removes anything not alphaumeric, tabs and newlines
            text = re.sub(r"[^A-Za-z0-9 ]","",wiki(link,option='No').gettext(outfull='yes').replace('\t', ' ').replace('\n', ''))
            if dropstubs == True:
                if 'stub' in text:
                    pass
                else:
                    export.write(text+','+'flagged'+'\n')
            else:
                export.write(text+','+'flagged'+'\n')

        
        i=0
        while i<int(len(flaggedlinks)*dataratio):
            # as unflagged category produces a random article when queried, it can be called repeatedly to generate multiple random articles
            text = re.sub(r"[^A-Za-z0-9 ]","",wiki(unflaggedcat,option='No').gettext(outfull='yes').replace('\t', ' ').replace('\n', ''))
            if dropstubs == True:
                if 'stub' in text:
                    pass
                else:
                    i+=1
                    export.write(text+','+'unflagged'+'\n')
            
            else:
                i+=1
                export.write(text+','+'unflagged'+'\n')

def _topwords(df,Status):
    # Internal function for extracting top 10 words from set of articles, used with func. inforreport
    top_N = 10
    stopwords = nltk.corpus.stopwords.words('english')
    # RegEx for stopwords
    RE_stopwords = r'\b(?:{})\b'.format('|'.join(stopwords))
    words = (df[df['Status']==Status].ArticleText
            .str.lower()
            .replace([r'\|', RE_stopwords], [' ', ''], regex=True)
            .str.cat(sep=' ')
            .split()
    )
    rslt = pd.DataFrame(Counter(words).most_common(top_N),\
        columns=['Word', 'Frequency']).set_index('Word')
    return rslt

def inforeport(file):
    '''
    Provides an info report on a file generated by func. datagen. Provides a histogram of article text
    length and outputs the top 10 words in flagged/unflagged articles

    file: csv file from datagen for analysis
    '''
    colnames=['ArticleText','Status'] 
    df = pd.read_csv(file, names=colnames, header=None)

    df['WordCount'] = df['ArticleText'].str.count(' ')+1
    sns.histplot(data=df, x="WordCount", hue="Status", stat='density')

    flgrslt = _topwords(df,'flagged')
    unflgrslt = _topwords(df,'unflagged')

    return (flgrslt, unflgrslt)

def modelmaker(datafile, picklefile, showstats=True):
    '''
    Generates machine learning model to characterize articles into flagged and unflagged categories.
    Uses a MultinomialNB as it was found to be the ebst through trial and error. Provides a 
    classificatiion report and outputs the model as a pickle file so it can be used to predict
    on other articles using func. articletester

    datafile: csv file from datagen for analysis
    picklefile: filename (ending in .pkl) to save learning model as
    showstats: Whether or not to output confusion matrix/classification report
    '''
    colnames=['ArticleText','Status'] 
    df = pd.read_csv(datafile, names=colnames, header=None)
    df.dropna(axis=0, inplace=True)
    X = df['ArticleText']
    y = df['Status']
    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,stratify=y)

    # Vectorizing and transforming after split to avoid data leakage
    cvec = TfidfVectorizer(stop_words='english', ngram_range=(1,2), min_df=5)
    ttrans = TfidfTransformer()
    X_train = cvec.fit_transform(X_train)
    X_train = ttrans.fit_transform(X_train)
    X_test = cvec.transform(X_test)
    X_test = ttrans.transform(X_test)

    nb=MultinomialNB()
    nb.fit(X_train,y_train)
    predictions = nb.predict(X_test)
    if showstats==True:
        print('\n Confusion matrix\n')
        print(confusion_matrix(y_test,predictions))
        print('\n')
        print(classification_report(y_test,predictions))

    file = open(picklefile, 'wb')
    pickle.dump(cvec, file)
    pickle.dump(ttrans, file)
    pickle.dump(nb, file)
    file.close()

def articletester(pagetitles,picklefile):
    '''
    Tests a selection of articles using the trained model to either be "flagged" or "unflagged"
    pagetitles: list of page in lst format (ex. ['Charles_III','Leif_Erikson')
    pickefile: picklefile containing the learning model
    '''
    file = open(picklefile, 'rb')
    cvec = pickle.load(file)
    ttrans = pickle.load(file)
    nb = pickle.load(file)
    file.close()
    articletext=[]
    for page in pagetitles:
        text = re.sub(r"[^A-Za-z0-9 ]","",wiki(page,option='No').gettext(outfull='yes').replace('\t', ' ').replace('\n', ''))
        articletext.append(text)
    X = cvec.transform(articletext)
    X = ttrans.transform(X)
    predictions=nb.predict(X)
    return(predictions)


#links = linkgen('Autobiography')
#datagen('datafile.csv',links,"Special:RandomInCategory/Living_people")

#model = modelmaker('datafile.csv','pickle.pkl')
#preds = articletester(['Charles_III','SisqÃ³','Nicoletta_Luciani','Grace_Deeb'],'pickle.pkl')
#print(preds)
#preds = articletester(['Charles_III'],model)

'''
(flgrslt, unflgrslt) = inforeport('datafile.csv')
print(flgrslt)
print(unflgrslt)

plt.figure(1)
flgrslt.plot.bar(rot=0, figsize=(8,5))
plt.title('Top words flagged articles')

plt.figure(2)
unflgrslt.plot.bar(rot=0, figsize=(8,5))
plt.title('Top words unflagged articles')
plt.show(block=True)
'''